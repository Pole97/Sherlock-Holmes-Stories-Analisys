---
title: "Analysis of the Sherlock Holmes Stories"
author: "Matteo Pol"
date: "11/1/2022"
output:
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: inline
---

```{r import, include=FALSE} 
library(dplyr)
library(tidytext)
library(tidyverse)
library(stringr)
library(wordcloud)
library(tibble)
library(stopwords)
library(plotly)
library(knitr)
library(corrplot)
library(igraph)
library(tidygraph)
library(ggraph)
library(widyr)
library(scales)
devtools::install_github("analyxcompany/resolution")
library(stm)
library(forcats)

```

## Introduction

```{r prepare novels, include=FALSE}
set.seed(2022)
#Loading dataset

colors <- c("#003f5c", "#7a5195", "#ef5675", "#ffa600")
houn <- read.delim("https://raw.githubusercontent.com/Pole97/Sherlock-Holmes-Stories-Analisys/main/novels/houn.txt", stringsAsFactors = FALSE, skip = 0, strip.white = TRUE) %>%
  rename(text = THE.HOUND.OF.THE.BASKERVILLES) %>%
  mutate(color = "blue") # THE HOUND OF THE BASKERVILLES

sign <- read.delim("https://raw.githubusercontent.com/Pole97/Sherlock-Holmes-Stories-Analisys/main/novels/sign.txt", stringsAsFactors = FALSE, skip = 0, strip.white = TRUE) %>%
  rename(text = THE.SIGN.OF.THE.FOUR) %>%
  mutate(color = "green") # THE SIGN OF THE FOUR

stud <- read.delim("https://raw.githubusercontent.com/Pole97/Sherlock-Holmes-Stories-Analisys/main/novels/stud.txt", stringsAsFactors = FALSE, skip = 0, strip.white = TRUE) %>%
  rename(text =  A.STUDY.IN.SCARLET) %>%
  mutate(color = "red") # A STUDY IN SCARLET

vall <- read.delim("https://raw.githubusercontent.com/Pole97/Sherlock-Holmes-Stories-Analisys/main/novels/vall.txt", stringsAsFactors = FALSE, skip = 0, strip.white = TRUE) %>%
  rename(text = THE.VALLEY.OF.FEAR) %>%
  mutate(color = "yellow") # THE VALLEY OF FEAR


#unnest tokens
prepare_data <- function(data, book){ data %>%
  mutate(book = book,
         linenumber = row_number(),
         chapter = cumsum(
           str_detect(text, regex("^(Chapter|CHAPTER) [\\divxlc]", ignore_case = TRUE))))}

houn <- prepare_data(houn, "The Hound of the Baskervilles")
sign <- prepare_data(sign, "The Sign of the Four")
stud <- prepare_data(stud, "A Study in Scarlet")
vall <- prepare_data(vall, "The Valley of Fear")

#tokens for every story and remove stopwords
houn_token <- houn %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) 

sign_token <- sign %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) 

stud_token <- stud %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) 

vall_token <- vall %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) 


novels_token <- houn_token %>%
  rbind(sign_token) %>%
  rbind(stud_token) %>%
  rbind(vall_token)

novels_raw <- houn %>%
  rbind(sign) %>%
  rbind(stud) %>%
  rbind(vall)
```

# Text mining

##### Frequency analysis with wordcloud 


```{r wordcloud_words, echo=FALSE, collapse=TRUE, warning=FALSE}

words_frequencies <- novels_token %>%
   count(word, sort = TRUE) 

wordcloud(words = words_frequencies$word, freq = words_frequencies$n, min.freq = 60,
          max.words = 70, random.order = FALSE, rot.per = 0.3, 
          colors=brewer.pal(8, "Dark2"))

```


```{r tf_idf_onegram, echo=FALSE, include=FALSE,collapse=TRUE, warning=FALSE}
tf_idf_onegram <- novels_token %>%
    count(color,book, word, sort = TRUE) %>%
    bind_tf_idf(word, book, n) %>%
    group_by(book) %>%
    top_n(10) %>%
    ungroup %>%
    mutate(word = reorder(word, tf_idf))
```
```{r tf_idf_onegram_plot, echo=FALSE, collapse=TRUE, warning=FALSE}
ggplot(tf_idf_onegram,aes(word, tf_idf, fill = book )) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ book, scales = "free") +
    scale_fill_manual(values = colors) +
    coord_flip() 
```


```{r most used bigrams, echo=FALSE, include=FALSE, collapse=TRUE, warning=FALSE}

novels_bigram <- novels_raw %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigrams_separated <- novels_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) 

bigram_counts <- bigrams_filtered %>% 
  count(book,word1, word2, sort = TRUE)%>%
  na.omit()

novels_counts_bigram <- bigram_counts %>%
  filter(n > 3) %>% #bigrams with at least 4 occurrences, in order to have lighter graphic
  unite("bigram", word1, word2, sep = " ") 


p <- novels_counts_bigram %>%
  ggplot(aes(x = factor(book, levels = c("A Study in Scarlet","The Sign of the Four","The Hound of the Baskervilles","The Valley of Fear")), y = n, color = book,
             text = paste0("Bigram: ", bigram,
                           "\n Occurrences: ", n))) +
  geom_point(position = position_jitter(width = 0.48)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "Books in cronological order",
        y = "Occourrences of bigram") +
  scale_color_manual(values = colors) +
  geom_hline(yintercept = 10, label = "Top 10", alpha = .8, linetype ="longdash", color = "#D0CE7C") 
```
```{r plot_bigrams, echo=FALSE,  collapse=TRUE, warning=FALSE}
font = list(
  size = 15,
  color = "white"
)

label = list(
  bordercolor = "transparent",
  font = font
)
ggplotly(p, tooltip = c("text")) %>%
  config(displayModeBar = FALSE) %>%
  style(hoverlabel = label) %>%
  layout(font = font,
         yaxis = list(fixedrange = TRUE),
         xaxis = list(fixedrange = TRUE))




```
```{r bigram as a network, echo=FALSE, collapse=TRUE, warning=FALSE}
bigram_graph  <- novels_counts_bigram %>%
  filter(n>7) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  select(word1, word2,n) %>%
  graph_from_data_frame()

#plot graph
# Using the ggraph function(), we plot the most frequent bi-grams as a network, or “graph” .

a <- grid::arrow(type = "closed", length = unit(1, "mm"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(0.5, "mm")) +
  geom_node_point(color = "lightblue", size = 1) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

```

```{r tf-idf bigram, echo=FALSE, collapse=TRUE, warning=FALSE}

book_tf_idf  <- novels_raw %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word) %>%
    filter(!word2 %in% stop_words$word)  %>%
    na.omit() %>%
    unite("bigram", word1, word2, sep = " ") %>%
    count(book, bigram, sort = TRUE) %>%
    bind_tf_idf(bigram, book, n)


book_tf_idf %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  scale_fill_manual(values = colors) +
  labs(x = "tf-idf", y = NULL)
```



#Sentiment analysis


```{r sentiment_general_analysis, echo=FALSE,include=FALSE, collapse=TRUE, warning=FALSE}

overall_sentiment <- novels_token %>%
    inner_join(get_sentiments("bing")) %>%
    count( sentiment,book)

#Most common Positive and negative words

sentiment_word_count <- novels_token %>%
    inner_join(get_sentiments("bing")) %>%
    count(word, sentiment)%>%
    ungroup()


plot_most_contribution <- sentiment_word_count %>%
    group_by(sentiment) %>%
    top_n(10) %>%
    ungroup() %>%
    mutate(word = reorder(word, n)) 

```
```{r sentiment_general_analysis_plot, echo=FALSE, collapse=TRUE, warning=FALSE}
ggplot(overall_sentiment, aes(as.character(book), n, fill=sentiment)) +   
    geom_bar(stat = "identity", position = 'dodge')+
    ggtitle("Negative and positive sentiments for each book") +
    xlab("Book") + ylab("No. of words")


ggplot(plot_most_contribution, aes(word, n, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~sentiment, scales = "free_y") +
    labs(y = "Contribution to sentiment",
         x = NULL) +
    coord_flip()
```



```{r sentiment_false_friends, echo=FALSE, collapse=TRUE, warning=FALSE}

AFINN <- get_sentiments("afinn")
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)


not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"")

negated_words <- c("not", "without", "don't", "won't")


novel_bigram_counts <- novels_bigram %>%
  count(book, bigram, sort = TRUE) %>%
  separate(bigram, c("word1", "word2"), sep = " ")

novel_bigram_counts %>%
  filter(word1 %in% negated_words) %>%
  count(word1, word2, wt = n, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  mutate(contribution = value * n) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 10) %>%
  ungroup() %>%
  mutate(word2 = reorder_within(word2, contribution, word1)) %>%
  ggplot(aes(contribution, word2, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free", nrow = 2) +
  scale_y_reordered() +
  labs(x = "Sentiment value * # of occurrences",
       y = "Words preceded by a negation")


```
``` {r sentiment_contribution, echo=FALSE, collapse=TRUE, warning=FALSE}

words_by_book <- novels_token %>%
  count(book, word, sort = TRUE) %>%
  ungroup()

top_sentiment_words <- words_by_book %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  mutate(contribution = value * n / sum(n))

top_sentiment_words %>%
  group_by(book) %>%
  slice_max(abs(contribution), n = 12) %>%
  ungroup() %>%
  mutate(book = reorder(book, contribution),
         word = reorder_within(word, contribution, book)) %>%
  ggplot(aes(contribution, word, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  facet_wrap(~ book, scales = "free") +
  labs(x = "Sentiment value contribution of each word in the book", y = NULL)

```

<p>Costruendo la matrice con gli indici di correlazione di Pearson relativi ad ogni coppia di storie, la maggior parte delle coppie mostrano una correlazione positiva debole, quindi vuol dire che, nonostante molti racconti siano di argomento simile, il lessico usato varia.Solo in un caso, tra <i>William Wilson</i> e <i>The Murders of the Rue Morgue</i>, si raggiunge lo 0,6.</p>
```{r correlation with Pearson, echo=FALSE, collapse=TRUE, warning=FALSE}

#non usiamo il set di parole completo, ma quello senza stop word
frequency <- words_by_book %>%
  group_by(book) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(book, proportion) 

frequency_matrix <- cor(frequency[, c(-1)], use = "pairwise.complete.obs") 

corrplot(frequency_matrix,method = 'circle', type = 'lower', insig='blank',
         addCoef.col ='black', number.cex = 0.8, diag=FALSE, tl.cex = 0.9, tl.pos='ld',tl.srt=45, cl.cex = 0.6)


ggplot(frequency, aes(x = `The Sign of the Four`, y = `A Study in Scarlet`)) +
  geom_abline(color = "gray40", lty = 2) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  labs(y = "A Study in Scarlet", x = "The Sign of the Four") +
  theme_light()

ggplot(frequency, aes(x = `The Valley of Fear`, y = `The Hound of the Baskervilles`)) +
  geom_abline(color = "gray40", lty = 2) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  labs(y = "The Valley of Fear", x = "The Hound of the Baskervilles") +
  theme_light()

```

#Examining Pairwise Correlation
```{r pairwise_prepare, echo=FALSE, include=FALSE, collapse=TRUE, warning=FALSE}
holmes_section_words <- novels_raw %>%
  mutate(section = row_number() %/% 10) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word)

# we need to filter for at least relatively common words first
word_cors <- holmes_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

frequent_words <- c("holmes", "house", "sir", "time")

top_corr_plot <- word_cors %>%
  filter(item1 %in% frequent_words) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()

```
```{r pairwise_most_occurred_words, echo=FALSE, collapse=TRUE, warning=FALSE}
top_corr_plot

g<-word_cors %>%
  filter(correlation > .3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") 


mcolor <- g$data %>% mutate(mcolor = if_else(name %in% frequent_words, 
                                     "red", "lightblue")) %>% select(mcolor)

g + geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(colour=mcolor$mcolor, size = 2) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

```


#Topic modeling
```{r topic modeling, echo=FALSE, collapse=TRUE, warning=FALSE}
sherlock_dfm <- novels_token %>%
    filter(word != "holmes") %>%
    count(book, word, sort = TRUE) %>%
    cast_dfm(book, word, n)

topic_model <- stm(sherlock_dfm, K = 2, 
                   verbose = FALSE, init.type = "Spectral")
#summary(topic_model)

td_beta <- tidy(topic_model)

td_beta %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Highest word probabilities for each topic",
         subtitle = "Different words are associated with different topics")


td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(sherlock_dfm))

ggplot(td_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE,bins=30) +
  facet_wrap(~ topic, ncol = 2) +
  labs(title = "Distribution of document probabilities for each topic",
       y = "Number of stories", x = expression(gamma))
```
